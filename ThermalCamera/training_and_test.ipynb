{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "input_tensor = tf.keras.Input(shape=(512,640,9))\n",
    "x = input_tensor\n",
    "def down(filters, apply_batchnorm=True):    \n",
    "    initializer = tf.random_normal_initializer(0.,0.02)\n",
    "    a = tf.keras.models.Sequential()\n",
    "    a.add(tf.keras.layers.Conv2D(filters=filters, kernel_size=(4,4), padding='same', strides=1, ))\n",
    "    a.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "    a.add(tf.keras.layers.BatchNormalization())\n",
    "    a.add(tf.keras.layers.LeakyReLU())\n",
    "    return a"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0e0c9bbe63bf468"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "down_stack = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f40c871c61c3583"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    b = down(64*(2**i))\n",
    "    down_stack.append(b)\n",
    "\n",
    "for i in range(3):\n",
    "    b = down(512)\n",
    "    down_stack.append(b)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90a87ba280249f6f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "last = x\n",
    "skips = []\n",
    "for c in down_stack:\n",
    "    last = c(last)\n",
    "    skips.append(last)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aea7c7551b55c656"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "skips = reversed(skips[:-1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27fa292391f6f4f8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def up(filters):\n",
    "    initializer = tf.random_normal_initializer(0.,.02)\n",
    "    tensor = tf.keras.models.Sequential()\n",
    "    tensor.add(tf.keras.layers.Conv2DTranspose(filters=filters, kernel_size=4, strides=2, padding='same', kernel_initializer=initializer))\n",
    "    tensor.add(tf.keras.layers.BatchNormalization())\n",
    "    tensor.add(tf.keras.layers.ReLU())\n",
    "    return tensor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af53ebbb50c0e885"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "up_stack = []"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2ce756f4ea0c70b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    up_stack.append(up(512))\n",
    "\n",
    "for i in range(3):\n",
    "    up_stack.append(up(512/(2**(i+1))))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1662248fbf355ae1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = last\n",
    "for u, skip in zip(up_stack, skips):\n",
    "    y = u(y)\n",
    "    y = tf.keras.layers.Concatenate()([y, skip])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41a73fe44df7adce"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "z = up(32)\n",
    "y = z(y)\n",
    "print(y.shape)\n",
    "y = tf.keras.layers.Concatenate()([x, y])\n",
    "y = tf.keras.layers.Conv2D(filters=3, kernel_size=4, strides=1, padding='same', activation='tanh')(y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c54aa2d329f6bed"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=input_tensor, outputs=y)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e695b5580c5324d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=input_tensor, outputs=y)\n",
    "model.summary()\n",
    "tf.keras.util.plot_model(model, show_shape=True, dpi=64)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75fc3988ffe661bb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "def preprocess(image):\n",
    "    image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "    image = (image/127.5) -1\n",
    "    retval = tf.expand_dims(image, axis=0)\n",
    "    return retval\n",
    "\n",
    "def concat_all_3(images):\n",
    "  return tf.concat(images, 3)\n",
    "\n",
    "def pad2(image): # get the image subtraction to a multiple of (640, 512)\n",
    "  border_h_0 = int((3072-2825)/2)\n",
    "  border_h_1 = border_h_0 + 1\n",
    "  width_crop_half = 17, 18\n",
    "  shape = image.shape\n",
    "  destination = np.zeros([3072, 3840, 3], dtype=np.uint8)\n",
    "  destination[123:3072-124, ::, ::] += image[::, 17:-18:,::]\n",
    "  return cv.resize(destination, (640,512))\n",
    "  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c25a8aa7e3245163"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "path = \"/home/computer/CS/DroneProject/im2im_labeling/images/\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e16ca90814bedff"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# only run the following code when you need to preprocess the images. this might take a while\n",
    "count = 30000\n",
    "\n",
    "for i in range(3993, count):\n",
    "  noir = pad2(cv.imread(path + \"unfiltered/NoIR_\" + str(i) + \".jpg\"))\n",
    "  optical = pad2(cv.imread(path + \"filtered/Optical_\" + str(i) + \".jpg\"))\n",
    "  subtraction = pad2(cv.imread(path + \"subtractions/subtraction_\" + str(i) + \".jpg\"))\n",
    "  #thermal = cv.imread(path + \"thermal/thermal_\" + str(i) + \".jpg\")\n",
    "  cv.imwrite(path+\"unfiltered_preprocessed/NoIR_\" + str(i) + \".jpg\", noir)\n",
    "  cv.imwrite(path+\"filtered_preprocessed/Optical_\" + str(i) + \".jpg\", optical)\n",
    "  cv.imwrite(path+\"subtractions_preprocessed/subtraction_\" + str(i) + \".jpg\", subtraction)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1748c949ec7a51c9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define the discriminator network\n",
    "discrim_input0 = tf.keras.layers.Input(shape=(512, 640, 3))\n",
    "discrim_input1 = tf.keras.layers.Input(shape=(512,640, 9))\n",
    "conc = tf.keras.layers.Concatenate()([discrim_input0, discrim_input1])\n",
    "x = conc\n",
    "initializer = tf.random_normal_initializer(0.0, .02)\n",
    "for i in range(4):\n",
    "    x = tf.keras.layers.Conv2D(filters=256*pow(2, -(i+1)), kernel_size=(3,3), padding='same', strides=1)(x)\n",
    "    x = tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "\n",
    "x = tf.keras.layers.BatchNormalization()(x)\n",
    "x = tf.keras.layers.LeakyReLU()(x)\n",
    "x = tf.keras.layers.Conv2D(filters=1, kernel_size=3, strides=1, padding='same', kernel_initializer=initializer)(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a22c6dae4cee566"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "discrim = tf.keras.Model(inputs=[discrim_input0, discrim_input1], outputs=x)\n",
    "discrim.summary()\n",
    "tf.keras.utils.plot_model(discrim, show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b56f074de7b20a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import random\n",
    "num_epochs= 30\n",
    "batch_size = 1 #construct a cv mat with given size python\n",
    "xentropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "l1 = lambda y_true, y_pred : tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "gen_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discrim_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=.5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c803ac822700461e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "batch_size = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d41a7f98c8d7e094"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def paths_for(i):\n",
    "    lst = []\n",
    "    lst.append(path+\"unfiltered_preprocessed/NoIR_\" + str(i) + \".jpg\")\n",
    "    lst.append(path+\"filtered_preprocessed/Optical_\" + str(i) + \".jpg\")\n",
    "    lst.append(path+\"subtractions_preprocessed/subtraction_\" + str(i) + \".jpg\")\n",
    "    lst.append(path + \"thermal/thermal_\" + str(i) + \".jpg\")\n",
    "    return lst"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9aa255e4765f3da6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "valid_indices = [i for i in range(0,3924)]\n",
    "for i in range(3993, 41772):\n",
    "    valid_indices.append(i)\n",
    "    \n",
    "for i in range(41978, 120000):\n",
    "    valid_indices.append(i)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c08d618a852f800"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "random.shuffle(valid_indices)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6229e05e1901cc58"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def preprocess_example(i):\n",
    "    paths_for_i = paths_for(i)\n",
    "    all_four = [preprocess(cv.imread(path)) for path in paths_for_i]\n",
    "    x = concat_all_3(all_four[:-1])\n",
    "    return x, all_four[-1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86a3fec707543b7c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def image_generator(image_paths): \n",
    "    for example in image_paths:\n",
    "        all_four = [preprocess(cv.imread(path2)) for path2 in example]\n",
    "        input_tensr = concat_all_3(all_four[:-1])\n",
    "        output = all_four[-1]\n",
    "        yield np.array(input_tensr), np.array(output)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6da250fab803a732"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "image_paths = [] # for the train data. hold out the other 20%\n",
    "for idx in valid_indices[:96000]:\n",
    "    image_paths.append(paths_for(idx))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "433f993bb5f1543e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# define the data set\n",
    "dataset = tf.data.Dataset.from_generator(lambda: image_generator(image_paths), output_types=(tf.float32, tf.float32), output_shapes=([None, 512, 640, 9], [None, 512, 640, 3]))\n",
    "#dataset = dataset.map(preprocess_example)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab5543b9cbf1e558"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import time\n",
    "def post_scale(im):\n",
    "    im = ((im*127.5) + 127.5).numpy()\n",
    "    return im"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a9c282e8e33b6dc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# WARNING: this code block loads weights into the model and the current weights are not guarenteed to be saved\n",
    "\n",
    "latest = 'epoch_6_12'\n",
    "model.load_weights(latest)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "332437a624f763ef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "l = 200\n",
    "l_w = 50\n",
    "l_b = l_w\n",
    "for epoch_i in range(0,num_epochs):\n",
    "    dataset.shuffle(buffer_size=10)\n",
    "    t0 = time.time()\n",
    "    epoch_gen_loss = 0.0\n",
    "    epoch_discrim_loss = 0.0\n",
    "    epoch_mse_loss = 0.0\n",
    "    batch_num = 0\n",
    "    with tf.device(\"/GPU:0\"):\n",
    "        for batch_x, batch_y in dataset:\n",
    "            if batch_num % 1000 == 0:\n",
    "                print(\"Batch number=\", batch_num)\n",
    "            with tf.GradientTape() as discrim_tape, tf.GradientTape() as gen_tape:\n",
    "                batch_gen_loss = 0.0\n",
    "                batch_discrim_loss = 0.0\n",
    "                for x, y in zip(batch_x, batch_y):\n",
    "                    #print(x.shape)\n",
    "                    y_pred = model(x, training=True)\n",
    "                    l2 = mse(y, y_pred)\n",
    "                    epoch_mse_loss += l2\n",
    "                    discrim_output_fake = discrim([y_pred, x], training=True)\n",
    "                    output_asif_real = tf.ones_like(discrim_output_fake)\n",
    "                    batch_gen_loss += l*l2 + xentropy(output_asif_real, discrim_output_fake)\n",
    "                    discrim_output_real = discrim([y, x], training=True)\n",
    "                    fake_logits = tf.zeros_like(discrim_output_real)\n",
    "                    batch_discrim_loss += xentropy(output_asif_real, discrim_output_real) + xentropy(fake_logits, discrim_output_fake)\n",
    "                \n",
    "                gen_grad = gen_tape.gradient(batch_gen_loss, model.trainable_variables)\n",
    "                gen_optimizer.apply_gradients(grads_and_vars=zip(gen_grad, model.trainable_variables))\n",
    "                \n",
    "                discrim_grad = discrim_tape.gradient(batch_discrim_loss, discrim.trainable_variables)\n",
    "                discrim_optimizer.apply_gradients(grads_and_vars=zip(discrim_grad, discrim.trainable_variables))\n",
    "                \n",
    "                epoch_gen_loss += batch_gen_loss\n",
    "                epoch_discrim_loss += batch_discrim_loss\n",
    "                \n",
    "                batch_num += 1\n",
    "            \n",
    "    print('Epoch ', epoch_i, 'Generator Loss: ', epoch_gen_loss.numpy()/(2*len(image_paths)), ' Discriminator Loss: ', epoch_discrim_loss.numpy()/(2*len(image_paths)), 'MSE loss: ', epoch_mse_loss.numpy()/(2*len(image_paths)))\n",
    "    print(\"Time taken: \", time.time()-t0)\n",
    "    model.save_weights('./epoch_'+str(epoch_i) + \"_13\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b0059d76ecef2b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_paths_for(i):\n",
    "    lst = []\n",
    "    lst.append(path+\"unfiltered_preprocessed/NoIR_\" + str(i) + \".jpg\")\n",
    "    lst.append(path+\"filtered_preprocessed/Optical_\" + str(i) + \".jpg\")\n",
    "    lst.append(path+\"subtractions_preprocessed/subtraction_\" + str(i) + \".jpg\")\n",
    "    lst.append(path + \"thermal/thermal_\" + str(i) + \".jpg\")\n",
    "    return lst"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ade76912c3376aef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_valid_indices = valid_indices[96000:]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90b475f588a8e2cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_image_paths = []\n",
    "for idx in test_valid_indices:\n",
    "    test_image_paths.append(test_paths_for(idx))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9970719aa553df0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_generator(lambda: image_generator(test_image_paths), output_types=(tf.float32, tf.float32), output_shapes=([None, 512, 640, 9], [None, 512, 640, 3]))\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5229ecde233e7ef4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def percent_difference(predicted, thermal):\n",
    "    predicted = predicted[0].numpy()\n",
    "    thermal = thermal[0].numpy()\n",
    "    abs_dif = cv.absdiff(predicted, thermal)\n",
    "    dif_r = abs_dif[::,::,2]\n",
    "    dif_g = abs_dif[::,::,1]\n",
    "    dif_b = abs_dif[::,::,0]\n",
    "    percent_dif_red = np.divide(dif_r, thermal[::,::, 2], out=np.zeros_like(dif_r), where=thermal[::,::,2]!=0)\n",
    "    percent_dif_red =  np.mean(percent_dif_red)\n",
    "    \n",
    "    percent_dif_green = np.divide(dif_g, thermal[::,::,1], out=np.zeros_like(dif_g), where=thermal[::,::,2]!=0)\n",
    "    percent_dif_green = np.mean(percent_dif_green)\n",
    "    \n",
    "    percent_dif_blue = np.divide(dif_b, thermal[::,::,0], out=np.zeros_like(dif_b), where=thermal[::,::,2]!=0)\n",
    "    percent_dif_blue = np.mean(percent_dif_blue)\n",
    "    \n",
    "    return [percent_dif_red, percent_dif_green, percent_dif_blue]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54e513f0723eccaa"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# run this code to test\n",
    "\n",
    "percent_diffs = [0.0, 0.0, 0.0]\n",
    "n = 0\n",
    "good = False\n",
    "for batch_x, batch_y in test_dataset:\n",
    "    if good:\n",
    "        break\n",
    "    for x,y in zip(batch_x, batch_y):\n",
    "        y_pred = model(x)\n",
    "        difs = percent_difference(y_pred, y)\n",
    "        percent_diffs[0] += difs[0]\n",
    "        percent_diffs[1] += difs[1]\n",
    "        percent_diffs[2] += difs[2]\n",
    "        n+=1\n",
    "        if n >= 5000:\n",
    "            good = True\n",
    "            \n",
    "        \n",
    "print(\"Average percent difference: \", [abs(percent_diff/n) for percent_diff in percent_diffs])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb7b966d61567201"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7372ab7dac3c62"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
